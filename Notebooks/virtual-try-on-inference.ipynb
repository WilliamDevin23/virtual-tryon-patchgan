{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Install Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install tensorflow==2.18.1 mediapipe numpy matplotlib opencv-python\n","!pip install neurite@git+https://github.com/adalca/neurite.git@40c6d0e277b12dc9dddb6e76f2dbdd373b7d22b1\n","!pip install voxelmorph@git+https://github.com/voxelmorph/voxelmorph.git@923a37d51b0c8d93eb576156c07ecb25c2a4e730"]},{"cell_type":"markdown","metadata":{},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import voxelmorph as vxm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2"]},{"cell_type":"markdown","metadata":{},"source":["# Import Images"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# REPLACE WITH YOUR IMAGE PATH\n","image_path = 'IMAGE_PATH'\n","# REPLACE WITH YOUR CLOTH IMAGE PATH\n","cloth_image_path = 'CLOTH_IMAGE_PATH'\n","\n","image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n","cloth_image = cv2.cvtColor(cv2.imread(cloth_image_path), cv2.COLOR_BGR2RGB)"]},{"cell_type":"markdown","metadata":{},"source":["# Utility Functions for Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!wget -O pose_landmarker.task -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_heavy/float16/1/pose_landmarker_heavy.task"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from mediapipe import solutions\n","from mediapipe.framework.formats import landmark_pb2\n","import numpy as np\n","\n","\n","def draw_landmarks_on_image(image_size, detection_result, image=None):\n","    pose_landmarks_list = detection_result.pose_landmarks\n","    if image is None :\n","        canvas = np.zeros(image_size)\n","    else :\n","        canvas = np.copy(image)\n","\n","    # Loop through the detected poses to visualize.\n","    for idx in range(len(pose_landmarks_list)):\n","        pose_landmarks = pose_landmarks_list[idx]\n","\n","        # Draw the pose landmarks.\n","        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n","        pose_landmarks_proto.landmark.extend([\n","          landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n","        ])\n","        solutions.drawing_utils.draw_landmarks(\n","          canvas,\n","          pose_landmarks_proto,\n","          solutions.pose.POSE_CONNECTIONS,\n","          solutions.drawing_styles.get_default_pose_landmarks_style())\n","    return canvas"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import mediapipe as mp\n","from mediapipe.tasks import python\n","from mediapipe.tasks.python import vision\n","\n","base_options = python.BaseOptions(model_asset_path='pose_landmarker.task')\n","options = vision.PoseLandmarkerOptions(\n","    base_options=base_options\n",")\n","detector = vision.PoseLandmarker.create_from_options(options)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_mediapipe_skeleton(image) :\n","    input_int = (image * 255).astype(np.uint8)\n","    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=input_int)\n","    detection_result = detector.detect(mp_image)\n","    landmark = draw_landmarks_on_image(input_int.shape, detection_result)\n","    landmark = np.expand_dims(np.mean(landmark, axis=-1), axis=-1)\n","    return np.array(landmark, dtype=np.float32) / 255"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_person_representations(image, size='small') :\n","    if size not in ['small', 'large'] :\n","        raise ValueError('Argument size must be either \"small\" or \"large\"')\n","    else :\n","        # standardize input shape\n","        image_small = cv2.resize(image, (192, 256))\n","        image_small = np.expand_dims(image_small, axis=0) / 255\n","\n","        image = cv2.resize(image, (384, 512))\n","        image = np.expand_dims(image, axis=0) / 255\n","    \n","        # get pose skeleton\n","        pose_skeleton = get_mediapipe_skeleton(image_small[0])\n","        pose_skeleton = np.expand_dims(pose_skeleton, axis=0)\n","    \n","        # predict agnostic segmentation\n","        agnostic_segmentation = agnostic_seg_model((image_small, pose_skeleton))\n","        agnostic_segmentation = np.argmax(agnostic_segmentation, axis=-1)\n","        agnostic_segmentation = np.expand_dims(np.where(agnostic_segmentation == 0, 0, 1), axis=-1).astype(np.float32)\n","    \n","        if size != 'small' :\n","            agnostic_segmentation = np.expand_dims(cv2.resize(agnostic_segmentation[0], (384, 512), interpolation=cv2.INTER_NEAREST), axis=-1)\n","            agnostic_segmentation = np.expand_dims(agnostic_segmentation, axis=0)\n","            agnostic_representation = image * agnostic_segmentation\n","            pose_skeleton = np.expand_dims(get_mediapipe_skeleton(image[0]), axis=0)\n","        else :\n","            agnostic_representation = image_small * agnostic_segmentation\n","        return agnostic_representation, pose_skeleton"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_cloth_segmentation(cloth_image, size='small') :\n","    if size not in ['small', 'large'] :\n","        raise ValueError('Argument size must be either \"small\" or \"large\"')\n","    else :\n","        cloth_image = cv2.resize(cloth_image, (192, 256))\n","        cloth_image = np.expand_dims(cloth_image, axis=0) / 255\n","        cloth_seg = clothes_seg_model(cloth_image)\n","        \n","        cloth_seg = np.where(cloth_seg > 0.5, 1, 0).astype(np.float32)\n","    \n","        if size != 'small' :\n","            cloth_seg = cv2.resize(cloth_seg[0], (384, 512), interpolation=cv2.INTER_NEAREST)\n","            cloth_seg = np.expand_dims(np.expand_dims(cloth_seg, axis=-1), axis=0)\n","    \n","        return cloth_seg"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def generate_tryon(image, cloth_image, size='small') :\n","    person_agnostic, pose_skeleton = get_person_representations(image, size=size)\n","    cloth_segmentation = get_cloth_segmentation(cloth_image, size=size)\n","\n","    if size == 'small' :\n","        cloth_image = cv2.resize(cloth_image, (192, 256))\n","    else :\n","        cloth_image = cv2.resize(cloth_image, (384, 512), interpolation=cv2.INTER_LINEAR)\n","    cloth_image = np.expand_dims(cloth_image, axis=0) / 255\n","\n","    if size == 'small' :\n","        deformation_fields = warp_unet_small((person_agnostic, pose_skeleton, cloth_image, cloth_segmentation))\n","        warped_cloth = vxm.layers.SpatialTransformer()([cloth_image, deformation_fields]).numpy()\n","        tryon = tryon_generator_small((person_agnostic, pose_skeleton, warped_cloth))\n","    else :\n","        deformation_fields = warp_unet_large((person_agnostic, pose_skeleton, cloth_image, cloth_segmentation))\n","        warped_cloth = vxm.layers.SpatialTransformer()([cloth_image, deformation_fields]).numpy()\n","        tryon = tryon_generator_large((person_agnostic, pose_skeleton, warped_cloth))\n","    return tryon[0]"]},{"cell_type":"markdown","metadata":{},"source":["# Import Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["agnostic_seg_model = tf.keras.models.load_model('/kaggle/input/viton-agnostic-segmentation/tensorflow2/v1/1/agnostic_segmentation.keras')\n","clothes_seg_model = tf.keras.models.load_model('/kaggle/input/viton-clothes-segmentation/tensorflow2/v1/1/clothes_segmentation.keras')\n","warp_unet_small = tf.keras.models.load_model('/kaggle/input/viton-small/tensorflow2/small/1/warp_unet.keras')\n","tryon_generator_small = tf.keras.models.load_model('/kaggle/input/viton-small/tensorflow2/small/1/tryon_generator.keras')\n","warp_unet_large = tf.keras.models.load_model('/kaggle/input/viton-large/tensorflow2/large/1/warp_unet_large.keras')\n","tryon_generator_large = tf.keras.models.load_model('/kaggle/input/viton-large/tensorflow2/large/1/tryon_generator_large.keras')"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tryon = generate_tryon(image, cloth_image)\n","tryon_large = generate_tryon(image, cloth_image, size='large')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(18, 8))\n","\n","plt.subplot(121)\n","plt.imshow(tryon)\n","plt.axis(False)\n","\n","plt.subplot(122)\n","plt.imshow(tryon_large)\n","plt.axis(False)\n","\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"isSourceIdPinned":true,"modelId":438424,"modelInstanceId":420793,"sourceId":551151,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":438421,"modelInstanceId":420790,"sourceId":551144,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":438389,"modelInstanceId":420759,"sourceId":551093,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":438351,"modelInstanceId":420719,"sourceId":551037,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
